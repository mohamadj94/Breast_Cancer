# -*- coding: utf-8 -*-
"""Breast Cancer Classification Model .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VH5DZJoS385tXT_AgwDmWFuSSDwqmONr

The **Breast Cancer Wisconsin** (Diagnostic) dataset consists of 569 observations of breast cancer cell features, with 30 features and a binary outcome variable (malignant or benign). The data was collected from digitized images of breast tissue, and the features represent the characteristics of each cell.

This dataset is a popular benchmark for machine learning classification tasks, and it has been used in a variety of studies to evaluate the performance of different machine learning algorithms. The dataset is often used to classify breast cancer as either malignant or benign, but it can also be used to classify breast cancer into different stages or types.

Here are some of the key features of the dataset:

* **The dataset consists** of 569 observations of breast cancer cell features.
The features represent the characteristics of each cell, such as the radius, texture, and perimeter.
* **The outcome variable** is binary, indicating whether the cancer is malignant or benign.
The dataset is available for download from the UCI Machine Learning Repository.

## **Gettig Data**

I used the Kaggle API to download the Breast Cancer Wisconsin Diagnostic Dataset. The API provides a simple way to access data from Kaggle without having to download it manually.
"""

!pip install -q kaggle

!mkdir ~/.kaggle/

!cp '/content/drive/MyDrive/Colab Notebooks/kaggle/kaggle.json' ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d utkarshx27/breast-cancer-wisconsin-diagnostic-dataset

!unzip /content/breast-cancer-wisconsin-diagnostic-dataset.zip

# import data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# lode data
df = pd.read_csv('/content/brca.csv')
df.sample(5)

df.shape

df.isnull().sum()

df.info()

df.columns

df = df.drop('Unnamed: 0', axis=1)
df.shape

df['y'].unique()

# Feature Engineering (One hot encoding)
df["y"] = pd.get_dummies(df['y'], drop_first=True)
df.sample(5)

df.describe()

# train_test_split
X = df.iloc[:, :-1].to_numpy()
y = df.iloc[:, -1].to_numpy()

print(X.shape)
print(y.shape)

# train phase
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""We want to use different machine learning models for the Breast Cancer Classification dataset to see which model can have the best performance. For this purpose, we will use the following models:

* Support Vector Machine (SVM)

* LinearRegression

* Gaussian

## **Support Vector Machine (SVM)**:

 SVM is a supervised learning algorithm that can be used for both classification and regression tasks. In classification tasks, SVM finds a hyperplane that separates the two classes of data with the largest possible margin. In regression tasks, SVM finds a function that best fits the data points with the smallest possible error.

## **SVM** ( **kernel = rbf** )
"""

#  data modeling
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# Create the SVM model with RBF kernel
# c = [10 , 100, 1000 , 10000] the best c = 10000
clf = SVC(kernel='rbf', C=10000)

# Train the model
clf.fit(X_train, y_train)

# Evaluate the model
print(clf.score(X_test, y_test))

"""## **SVM** ( **kernel = linear** )"""

# Create the SVM model with linear kernel
# c = [10 , 100, 1000 , 10000] the best c = 10000
clf = SVC(kernel='linear', C=10000)

# Train the model
clf.fit(X_train, y_train)

# Evaluate the model
print(clf.score(X_test, y_test))

"""## **Linear Regression:**

Linear regression is a supervised learning algorithm that can be used to predict a continuous value from a set of features. The model learns a linear relationship between the features and the target value.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Create the LinearRegression
model = LinearRegression()

# creat the hyperparametr grid
param_grid = {'fit_intercept': [True, False]}

# Create the grid search
grid_search = GridSearchCV(model, param_grid, cv=5)

# Fit the grid search
grid_search.fit(X_train, y_train)

# make pridictions
predictions = grid_search.predict(X_test)

# calculate the mean_squared_error
mse = mean_squared_error(y_test, predictions)

# print best pararmetrs and Mean squar error
print(f'best pararmetrs:, {grid_search.best_params_}')
print(f'Mean squar error: , {mse}')

r2_score(y_test, predictions)

percent_accuracy = (1 - (mse / y_test.mean())) * 100
print(f"Model Accuracy: {percent_accuracy}%")

"""## **Gaussian Process Regression:**

 Gaussian process regression is a non-parametric supervised learning algorithm that can be used to predict a continuous value from a set of features. The model learns a Gaussian process, which is a probability distribution over functions.
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV


param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]}

# Create the GaussianNB
gnb = GaussianNB()

# hyperparametr Tuning
clf = GridSearchCV(gnb, param_grid, cv=5)

# fitting the model
clf.fit(X_train, y_train)

print(clf.best_params_)

# Create the model
gnb = GaussianNB(var_smoothing=1e-09)

# Fit the model to the data
gnb.fit(X_train, y_train)

# Evaluate the model on the test data
score = gnb.score(X_test, y_test)
print(score)